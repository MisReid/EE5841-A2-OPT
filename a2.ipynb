{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa844161",
   "metadata": {},
   "source": [
    "# Assignment 2 Starter Notebook (TensorFlow/Keras)\n",
    "## Training Deep Neural Networks: Initialization, Regularization, and Optimization (Week 3)\n",
    "\n",
    "**Lectures covered:** Week 3 (gradient flow, Xavier/He init, L2/Dropout/BatchNorm, SGD/Momentum, Adam/RMSProp/Adagrad)\n",
    "\n",
    "**Dataset:** UCI Adult Income (1994 U.S. Census) via OpenML\n",
    "\n",
    "> This notebook is a starter template. Complete all **TODO** sections and keep experiments controlled (change one factor at a time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa09354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 21:10:59.986312: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-10 21:10:59.986339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-10 21:10:59.990155: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-10 21:11:00.003460: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-10 21:11:01.248871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa16506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.15.1\n"
     ]
    }
   ],
   "source": [
    "# === Reproducibility ===\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d706cf",
   "metadata": {},
   "source": [
    "## Q1. Dataset Loading (Ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e90ef-82c9-4385-8046-738e3ecf3532",
   "metadata": {},
   "source": [
    "### Load Adult Income Dataset (OpenML)\n",
    "\n",
    "This cell loads the **Adult (Census Income)** dataset from OpenML using `fetch_openml` and separates features and labels.  \n",
    "- **X** contains the input features (demographic and employment attributes).  \n",
    "- **y** contains the target labels indicating whether annual income is **>50K** or **≤50K**.  \n",
    "The shapes and a preview of the feature table are printed for quick inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82537be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (48842, 14)\n",
      "y shape: (48842,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  education-num      marital-status  \\\n",
       "0   25    Private  226802          11th              7       Never-married   \n",
       "1   38    Private   89814       HS-grad              9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm             12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college             10  Married-civ-spouse   \n",
       "4   18        NaN  103497  Some-college             10       Never-married   \n",
       "\n",
       "          occupation relationship   race     sex  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                NaN    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country  \n",
       "0              40  United-States  \n",
       "1              50  United-States  \n",
       "2              40  United-States  \n",
       "3              40  United-States  \n",
       "4              30  United-States  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required access method:\n",
    "adult = fetch_openml(name=\"adult\", version=2, as_frame=True)\n",
    "\n",
    "X = adult.data\n",
    "y = adult.target  # strings: '>50K' or '<=50K'\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b9163-a1e4-4668-bcd5-eb33b57273b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ✅ Student Instructions (Start Here)\n",
    "\n",
    "Your work begins in the **next code cells (Q2–Q10)** and continues by answering questions in the **Markdown cells (Q11–Q14)**. These correspond to the questions listed in the assignment description on Canvas. Complete each cell by following the instructions provided in the **preceding Markdown cells**.\n",
    "\n",
    "Please:\n",
    "- **Read the instructions carefully** before you begin coding.\n",
    "- Take time to **understand each question** and implement the required steps.\n",
    "- Each code cell includes **partial starter code**—your task is to **fill in the missing parts** and ensure the cell runs correctly.\n",
    "\n",
    "If you need clarification at any point, please contact the **teaching staff (instructor/TA)** for support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844affc",
   "metadata": {},
   "source": [
    "## Q2. Train/Validation/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b9dda-75c2-4663-9852-c3d72161dd75",
   "metadata": {},
   "source": [
    "### Train / Validation / Test Split (Stratified)\n",
    "\n",
    "In this step, you will split the dataset into **training (60%)**, **validation (20%)**, and **test (20%)** sets.\n",
    "\n",
    "- Use a **two-stage split** to achieve the 60/20/20 ratio.\n",
    "- Apply **stratification by the target label** to preserve the original class distribution in all splits.\n",
    "- Fix the **random seed (`SEED`)** to ensure reproducibility.\n",
    "- Print the shapes of each split to verify that the proportions are correct.\n",
    "\n",
    "This split allows you to train the model on the training set, tune hyperparameters using the validation set, and report final performance on the unseen test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbf1b36-ebd3-4f05-b051-9a7fff2ec3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (29305, 14) Val: (15629, 14) Test: (3908, 14)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use a 60/20/20 split (train/val/test), stratified by label.\n",
    "\n",
    "# Step 1: Split the data into training (60%) and temporary (40%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.4,            # TODO: fraction for validation+test\n",
    "    random_state=SEED ,         # TODO: set the random seed\n",
    "    stratify=y               # TODO: stratify by the target labels\n",
    ")\n",
    "\n",
    "# Step 2: Split the temporary set equally into validation (20%) and test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.2 ,            # TODO: fraction for test split\n",
    "    random_state=SEED ,         # TODO: set the same random seed\n",
    "    stratify=y_temp               # TODO: stratify by the temporary labels\n",
    ")\n",
    "\n",
    "# Verify split sizes\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052327e",
   "metadata": {},
   "source": [
    "## Q3. Preprocessing (one-hot encode categoricals; scale numericals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d17ae-51cf-4b29-a168-216340156709",
   "metadata": {},
   "source": [
    "### Feature Engineering & Preprocessing (What You Should Do)\n",
    "\n",
    "In this step, you will prepare the dataset for machine learning by handling **categorical and numerical features appropriately**.\n",
    "\n",
    "You should:\n",
    "- **Identify categorical and numerical columns** in the training set using data types.\n",
    "- Apply **One-Hot Encoding (OHE)** to categorical features, ensuring unseen categories are safely handled.\n",
    "- Apply **Standard Scaling** to numerical features so they are on a comparable scale.\n",
    "- Use a **ColumnTransformer** to combine these preprocessing steps into a single pipeline.\n",
    "- **Fit the preprocessing pipeline only on the training data**, then use it to transform the validation and test sets (to avoid data leakage).\n",
    "- **Encode the target labels** into binary values `{0, 1}`, where `1` corresponds to income `>50K`.\n",
    "\n",
    "Finally, verify your work by printing the **processed feature shapes** for the training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b4ca014-2156-4c32-8be6-cc7d88c4f2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: 8\n",
      "Numeric columns: 6\n",
      "Processed shapes: (29305, 108) (15629, 108) (3908, 108)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Identify categorical and numerical feature columns from the training set\n",
    "\n",
    "#print(X_train.dtypes)\n",
    "\n",
    "cat_cols = X_train.select_dtypes(\n",
    "    include=[\"category\"]          # TODO: data types for categorical features\n",
    ").columns.tolist()\n",
    "\n",
    "num_cols = X_train.select_dtypes(\n",
    "    exclude=[\"category\"]          # TODO: exclude categorical feature types\n",
    ").columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", len(cat_cols))\n",
    "print(\"Numeric columns:\", len(num_cols))\n",
    "\n",
    "\n",
    "# TODO: Build a preprocessing pipeline\n",
    "# - Apply StandardScaler to numerical features\n",
    "# - Apply OneHotEncoder to categorical features\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),   # TODO: scaler for numerical columns\n",
    "        (\"cat\", OneHotEncoder(\n",
    "            handle_unknown=\"ignore\",\n",
    "            sparse_output=False        # TODO: return dense output\n",
    "        ), cat_cols),\n",
    "    ],\n",
    "    remainder='drop'                  # TODO: drop unused columns\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Fit preprocessing on TRAIN only, then transform all splits\n",
    "X_train_p = preprocess.fit_transform(X_train)\n",
    "X_val_p   = preprocess.transform(X_val)\n",
    "X_test_p  = preprocess.transform(X_test)\n",
    "\n",
    "\n",
    "# TODO: Encode labels into {0, 1}\n",
    "# 1 corresponds to income >50K\n",
    "y_train_i = (y_train == y_train).astype(\"int32\").to_numpy()\n",
    "y_val_i   = (y_val   == y_val).astype(\"int32\").to_numpy()\n",
    "y_test_i  = (y_test  == y_test).astype(\"int32\").to_numpy()\n",
    "\n",
    "# Verify processed feature shapes\n",
    "print(\"Processed shapes:\", X_train_p.shape, X_val_p.shape, X_test_p.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037da94",
   "metadata": {},
   "source": [
    "## Q4. Deep MLP definition (≥ 6 hidden layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0ace5d-e48e-4dc1-84c5-2bca65b1a5a0",
   "metadata": {},
   "source": [
    "### Build a Deep MLP Model (What You Should Do)\n",
    "\n",
    "In this step, you will **construct a deep multilayer perceptron (MLP)** for binary classification using Keras.\n",
    "\n",
    "You should:\n",
    "- Use the provided `build_deep_mlp` function to **define a configurable deep neural network**.\n",
    "- Specify the **input dimension** based on the number of preprocessed features.\n",
    "- Control the **model depth and width** to create a deep architecture suitable for the task.\n",
    "- Optionally enable **Batch Normalization, Dropout, and L2 regularization** to improve training stability and generalization.\n",
    "- Ensure the **final output layer** uses a sigmoid activation for binary classification.\n",
    "\n",
    "After building the model:\n",
    "- Call `model.summary()` to **inspect the network architecture**, including layer types, output shapes, and number of trainable parameters.\n",
    "\n",
    "This step helps you understand how architectural choices (depth, width, regularization) affect deep neural network design.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a5e1534-a889-4cda-a5fe-11f9b11d53a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 108)]             0         \n",
      "                                                                 \n",
      " dense_378 (Dense)           (None, 256)               27904     \n",
      "                                                                 \n",
      " batch_normalization_356 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_356 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_356 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_379 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_357 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_357 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_357 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_380 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_358 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_358 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_358 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_381 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_359 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_359 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_359 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_360 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_360 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_360 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_361 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_361 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_361 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_384 (Dense)           (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 365578 (1.39 MB)\n",
      "Trainable params: 362506 (1.38 MB)\n",
      "Non-trainable params: 3072 (12.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete the deep MLP builder with configurable depth, width, and regularization\n",
    "\n",
    "def build_deep_mlp(\n",
    "    input_dim: int,\n",
    "    depth: int = 5,                     # TODO: number of hidden layers\n",
    "    width: int = 64,                     # TODO: number of units per layer\n",
    "    activation: str = \"relu\",                # TODO: hidden-layer activation\n",
    "    kernel_initializer: str | keras.initializers.Initializer = \"random_normal\",\n",
    "    use_batchnorm: bool = 1,             # TODO: enable/disable BatchNorm\n",
    "    dropout_rate: float = 0.2,             # TODO: dropout probability\n",
    "    l2_weight: float = 0.2,                # TODO: L2 regularization strength\n",
    ") -> keras.Model:\n",
    "    \"\"\"Deep MLP for binary classification with configurable init, BN, dropout, and L2.\"\"\"\n",
    "\n",
    "    # TODO: Add L2 regularization only if l2_weight > 0\n",
    "    reg = keras.regularizers.L2(l2_weight) if 1 and l2_weight > 0 else None\n",
    "\n",
    "    # TODO: Define model input\n",
    "    inputs = keras.Input(shape=(input_dim ,))\n",
    "    x = inputs\n",
    "\n",
    "    # TODO: Build hidden layers\n",
    "    for j in range(depth):\n",
    "        x = layers.Dense(\n",
    "            width,                           # TODO: layer width\n",
    "            activation=None,               # TODO: no activation here\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            kernel_regularizer=reg ,\n",
    "        )(x)\n",
    "\n",
    "        # TODO: Optional Batch Normalization\n",
    "        if use_batchnorm:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # TODO: Apply activation\n",
    "        x = layers.Activation(activation)(x)\n",
    "\n",
    "        # TODO: Optional Dropout\n",
    "        if 1 and dropout_rate > 0:\n",
    "            x = layers.Dropout (dropout_rate)(x)\n",
    "\n",
    "    # TODO: Output layer for binary classification\n",
    "    outputs = layers.Dense(10, activation=activation)(x)\n",
    "\n",
    "    # TODO: Build and return the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# function call- do not change\n",
    "input_dim = X_train_p.shape[1]\n",
    "deep_model = build_deep_mlp(input_dim=input_dim, depth=6, width=256)\n",
    "deep_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ed1b3",
   "metadata": {},
   "source": [
    "## Q5. Gradient flow diagnostics (vanishing/exploding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af57e52-2755-49db-9ce2-66d6b00ff029",
   "metadata": {},
   "source": [
    "### Measure Vanishing/Exploding Gradients (What You Should Do)\n",
    "\n",
    "In this step, you will **diagnose gradient behavior** in a deep MLP by computing **layer-wise gradient norms**.\n",
    "\n",
    "You should:\n",
    "- Use the provided helper function `layerwise_grad_norms(...)` to compute:\n",
    "  - the **batch loss**, and  \n",
    "  - the **L2 norm of the gradient** for each Dense layer’s **kernel weights**.\n",
    "- Create a small mini-batch (`BATCH = 512`) from the preprocessed training data.\n",
    "- Build a deep model using a **clearly “naïve” weight initialization** (e.g., `RandomNormal(stddev=1.0)`), which can increase the risk of unstable gradients.\n",
    "- Run one forward pass to ensure the model weights are created, then compute and print:\n",
    "  - the **initial batch loss**\n",
    "  - the **gradient norms** for early layers (and optionally all layers)\n",
    "\n",
    "**Your key task for analysis:**\n",
    "- During training (across epochs), **record these gradient norms** and observe whether they:\n",
    "  - shrink toward zero (**vanishing gradients**), or\n",
    "  - grow very large (**exploding gradients**),\n",
    "  especially in earlier layers.\n",
    "\n",
    "You will use these observations in your written answers to explain *why* training becomes difficult in deep networks and *how* techniques like better initialization, BatchNorm, or residual connections can help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f31a9dc8-8d46-4949-8a7d-628182132ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial batch loss: 8.374911308288574\n",
      "dense_385/kernel:0 -> 0.6092112064361572\n",
      "dense_386/kernel:0 -> 0.489471435546875\n",
      "dense_387/kernel:0 -> 0.3910927176475525\n",
      "dense_388/kernel:0 -> 0.33130744099617004\n",
      "dense_389/kernel:0 -> 0.3037320077419281\n",
      "dense_390/kernel:0 -> 0.287049800157547\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete the utility that computes layer-wise gradient norms for Dense kernels\n",
    "\n",
    "def layerwise_grad_norms(model: keras.Model, x_batch, y_batch):\n",
    "    # TODO: convert x_batch to a float32 tensor\n",
    "    x_batch = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "\n",
    "    # TODO: reshape y_batch to (N, 1) and convert to float32 tensor\n",
    "    y_batch = tf.convert_to_tensor(y_batch.reshape(y_batch.size, -1), dtype=tf.float32)\n",
    "\n",
    "    # TODO: compute loss under a GradientTape context\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_batch, training=True)   # TODO: training flag\n",
    "        loss = keras.losses.BinaryCrossentropy(from_logits=False)  # TODO: binary cross-entropy\n",
    "        loss = tf.reduce_mean(loss(y_batch, y_pred))\n",
    "\n",
    "    # TODO: compute gradients of loss w.r.t. trainable variables\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # TODO: collect gradient norms for Dense layer kernels only\n",
    "    norms = []\n",
    "    names = []\n",
    "    for var, g in zip(model.trainable_variables, grads):\n",
    "        if g is None:\n",
    "            continue\n",
    "        if \"dense\" in var.name and \"kernel\" in var.name:\n",
    "            norms.append(float(tf.norm(g)))   # TODO: gradient tensor\n",
    "            names.append(var.name )                  # TODO: variable name\n",
    "\n",
    "    return float(loss), names, norms\n",
    "\n",
    "\n",
    "# TODO: Prepare a mini-batch for gradient diagnostics\n",
    "BATCH = 512\n",
    "x_b = X_train_p[:BATCH]\n",
    "y_b = y_train_i[:BATCH]\n",
    "\n",
    "\n",
    "# TODO: Create a model with a clearly \"naïve\" initialization for Q1\n",
    "naive_init = keras.initializers.RandomNormal(\n",
    "    mean=0,\n",
    "    stddev=1,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "model_naive = build_deep_mlp(\n",
    "    input_dim=input_dim, \n",
    "    depth=6, \n",
    "    width=256,\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=naive_init\n",
    ")\n",
    "\n",
    "# TODO: Run a forward pass once to build weights\n",
    "_ = model_naive(tf.convert_to_tensor(x_b, dtype=tf.float32))\n",
    "\n",
    "# function call- do not change\n",
    "loss0, names0, norms0 = layerwise_grad_norms(model_naive, x_b, y_b)\n",
    "print(\"Initial batch loss:\", loss0)\n",
    "for n, v in list(zip(names0, norms0))[:6]:\n",
    "    print(n, \"->\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809bfe5a",
   "metadata": {},
   "source": [
    "## Q6. Initialization study (Xavier vs He)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4840f-bb9f-49e0-b7b3-78904e899cd0",
   "metadata": {},
   "source": [
    "### Compare Xavier vs. He Initialization (What You Should Do)\n",
    "\n",
    "In this step, you will **train two deep MLP models under controlled settings** to compare how different weight initializations affect learning.\n",
    "\n",
    "You should:\n",
    "- Use `compile_model(...)` to compile each model with:\n",
    "  - **binary cross-entropy** loss\n",
    "  - **binary accuracy** metric\n",
    "  - the **same optimizer** (SGD) and learning rate for a fair comparison\n",
    "- Train both models using `fit_model(...)` with the same:\n",
    "  - number of epochs (e.g., 20)\n",
    "  - batch size (e.g., 256)\n",
    "\n",
    "Models to compare:\n",
    "- **Xavier/Glorot initialization** with **tanh** activation  \n",
    "- **He initialization** with **ReLU** activation  \n",
    "\n",
    "After training:\n",
    "- Plot and compare **validation loss vs. epoch** for both models.\n",
    "- Interpret which combination converges faster and achieves lower validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e4bab83-5944-465e-92f1-a5097ec42db1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/metrics/accuracy_metrics.py\", line 361, in accuracy  **\n        y_true.shape.assert_is_compatible_with(y_pred.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 54\u001b[0m\n\u001b[1;32m     42\u001b[0m model_he \u001b[38;5;241m=\u001b[39m compile_model(\n\u001b[1;32m     43\u001b[0m     build_deep_mlp(\n\u001b[1;32m     44\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# TODO: same optimizer and lr\u001b[39;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# function call- do not change\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m hist_xavier \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_xavier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m hist_he \u001b[38;5;241m=\u001b[39m fit_model(model_he, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     57\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model, epochs, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_model\u001b[39m(model: keras\u001b[38;5;241m.\u001b[39mModel, epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# TODO: training features and labels\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_i\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# TODO: validation features and labels\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# TODO: 0 for silent training\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileugaktl_g.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/cozy/miniconda3/envs/CS_5841_ML/lib/python3.10/site-packages/keras/src/metrics/accuracy_metrics.py\", line 361, in accuracy  **\n        y_true.shape.assert_is_compatible_with(y_pred.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete helper functions to compile and train a Keras model\n",
    "\n",
    "def compile_model(model: keras.Model, optimizer: keras.optimizers.Optimizer):\n",
    "    model.compile(\n",
    "        optimizer=optimizer,                         # TODO: pass the optimizer\n",
    "        loss='binary_crossentropy',                              # TODO: binary classification loss\n",
    "        metrics=[keras.metrics.Accuracy(name='accuracy')  # TODO: accuracy metric + name\n",
    "               ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(model: keras.Model, epochs: int = 20, batch_size: int = 256):\n",
    "    history = model.fit(\n",
    "        X_train_p, y_train_i,                            # TODO: training features and labels\n",
    "        validation_data=(X_val_p, y_val_i),           # TODO: validation features and labels\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0                            # TODO: 0 for silent training\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "# TODO: Define Xavier (Glorot) and He initializations\n",
    "xavier = keras.initializers.GlorotUniform(seed=SEED)\n",
    "he     = keras.initializers.HeNormal(seed=SEED)\n",
    "\n",
    "\n",
    "# TODO: Build + compile two controlled models:\n",
    "# 1) Xavier + tanh, 2) He + ReLU. Use the same optimizer type and learning rate.\n",
    "model_xavier = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=5,\n",
    "        width=256,\n",
    "        activation='tanh',                        # TODO: tanh\n",
    "        kernel_initializer=xavier\n",
    "    ),\n",
    "    keras.optimizers.SGD(learning_rate=0.001)  # TODO: SGD and lr\n",
    ")\n",
    "\n",
    "model_he = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=65,\n",
    "        width=256,\n",
    "        activation='relu',                        # TODO: relu\n",
    "        kernel_initializer=he\n",
    "    ),\n",
    "    keras.optimizers.SGD(learning_rate=0.001)  # TODO: same optimizer and lr\n",
    ")\n",
    "\n",
    "# function call- do not change\n",
    "hist_xavier = fit_model(model_xavier, epochs=20)\n",
    "hist_he = fit_model(model_he, epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_xavier.history[\"val_loss\"], label=\"Xavier (tanh) val_loss\")\n",
    "plt.plot(hist_he.history[\"val_loss\"], label=\"He (relu) val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Initialization comparison (controlled)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4496c0",
   "metadata": {},
   "source": [
    "## Q7. Regularization study (L2 and Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72026fe-db28-494b-a522-8a08e416b2e2",
   "metadata": {},
   "source": [
    "### Regularization Experiments: L2 Weight Decay vs. Dropout (What You Should Do)\n",
    "\n",
    "In this step, you will test how **regularization** affects overfitting and generalization in a deep MLP.\n",
    "\n",
    "You should:\n",
    "- **Choose ONE best initialization from Q2** (Xavier or He) and keep it fixed for all experiments here to ensure a fair comparison.\n",
    "  - Update `base_init = ...` based on your Q2 results.\n",
    "\n",
    "Then run two controlled studies:\n",
    "\n",
    "**1) L2 Weight Decay (Kernel Regularizer)**\n",
    "- Train two models with different L2 strengths:\n",
    "  - `l2_weight = 1e-4`\n",
    "  - `l2_weight = 1e-3`\n",
    "- Keep architecture, optimizer, learning rate, epochs, and batch size the same.\n",
    "\n",
    "**2) Dropout**\n",
    "- Train two models with different dropout rates:\n",
    "  - `dropout_rate = 0.2`\n",
    "  - `dropout_rate = 0.5`\n",
    "- Keep everything else the same as in the L2 study.\n",
    "\n",
    "After training:\n",
    "- Plot **validation loss vs. epoch** for all four models on the same figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b5691-a640-4450-acdd-dfe7b3cfc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Regularization Experiments — L2 Weight Decay and Dropout\n",
    "\n",
    "# TODO: Choose ONE best initialization from Q7 and keep it fixed\n",
    "base_init = keras.initializers.GlorotUniform(seed=SEED)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# L2 Weight Decay Study\n",
    "# =========================\n",
    "\n",
    "#variables used for all models\n",
    "lrate = 0.02  #learning rate\n",
    "d, w = 6, 256 #depth, width\n",
    "m = 0.8       #momentum\n",
    "activ = 'relu'#activation function\n",
    "\n",
    "model_l2_1 = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=d,\n",
    "        width=w,\n",
    "        activation=activ                 # TODO: activation function\n",
    "        kernel_initializer=base_init,\n",
    "        l2_weight=0.0001                   # TODO: smaller L2 value (e.g., 1e-4)\n",
    "    ),\n",
    "    keras.optimizers.SGD(              # TODO: optimizer type\n",
    "        learning_rate=lrate,\n",
    "        momentum=m                   # TODO: momentum value\n",
    "    )\n",
    ")\n",
    "\n",
    "model_l2_2 = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=6,\n",
    "        width=256,\n",
    "        activation=activ,\n",
    "        kernel_initializer=base_init,\n",
    "        l2_weight=0.001                   # TODO: larger L2 value (e.g., 1e-3)\n",
    "    ),\n",
    "    keras.optimizers.SGD(\n",
    "        learning_rate=lrate,\n",
    "        momentum=m\n",
    "    )\n",
    ")\n",
    "\n",
    "# function call- do not change\n",
    "hist_l2_1 = fit_model(model_l2_1, epochs=20)\n",
    "hist_l2_2 = fit_model(model_l2_2, epochs=20)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Dropout Study\n",
    "# =========================\n",
    "\n",
    "model_do_1 = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=d,\n",
    "        width=w,\n",
    "        activation=activ,\n",
    "        kernel_initializer=base_init,\n",
    "        dropout_rate=0.2               # TODO: moderate dropout (e.g., 0.2)\n",
    "    ),\n",
    "    keras.optimizers.SGD(\n",
    "        learning_rate=lrate,\n",
    "        momentum=m\n",
    "    )\n",
    ")\n",
    "\n",
    "model_do_2 = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=d,\n",
    "        width=w,\n",
    "        activation=activ,\n",
    "        kernel_initializer=base_init,\n",
    "        dropout_rate=0.5               # TODO: stronger dropout (e.g., 0.5)\n",
    "    ),\n",
    "    keras.optimizers.SGD(\n",
    "        learning_rate=lrate,\n",
    "        momentum=m\n",
    "    )\n",
    ")\n",
    "\n",
    "# function call- do not change\n",
    "hist_do_1 = fit_model(model_do_1, epochs=20)\n",
    "hist_do_2 = fit_model(model_do_2, epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_l2_1.history[\"val_loss\"], label=\"L2=1e-4 val_loss\")\n",
    "plt.plot(hist_l2_2.history[\"val_loss\"], label=\"L2=1e-3 val_loss\")\n",
    "plt.plot(hist_do_1.history[\"val_loss\"], label=\"Dropout=0.2 val_loss\")\n",
    "plt.plot(hist_do_2.history[\"val_loss\"], label=\"Dropout=0.5 val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Regularization comparison (L2 vs Dropout)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979d8e9",
   "metadata": {},
   "source": [
    "## Q8. Batch Normalization study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a63dc-2332-4310-8094-a0ff011a4325",
   "metadata": {},
   "source": [
    "### Batch Normalization Experiment (What You Should Do)\n",
    "\n",
    "In this step, you will evaluate how **Batch Normalization (BN)** affects training stability and convergence in a deep MLP.\n",
    "\n",
    "You should:\n",
    "- Keep the setup **controlled and consistent**:\n",
    "  - Use the same **base initialization** (`base_init`) you selected earlier.\n",
    "  - Use the same **optimizer and learning rate** (SGD, `lr=1e-2`, no momentum).\n",
    "  - Use the same architecture (depth, width, activation).\n",
    "\n",
    "Train two models:\n",
    "- **With BatchNorm**: `use_batchnorm=True`\n",
    "- **Without BatchNorm**: `use_batchnorm=False`\n",
    "\n",
    "After training:\n",
    "- Plot **validation loss vs. epoch** for both models on the same figure.\n",
    "- Compare which model:\n",
    "  - converges faster,\n",
    "  - has smoother/more stable learning,\n",
    "  - achieves better final validation loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ba9a8-1aa8-433e-b83e-afc015b59a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: BatchNorm Experiment — keep initialization + optimizer controlled\n",
    "\n",
    "# Build + compile model WITH Batch Normalization\n",
    "model_bn = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=d,\n",
    "        width=w,\n",
    "        activation=activ,                  # TODO: activation function\n",
    "        kernel_initializer=base_init,          # TODO: base_init\n",
    "        use_batchnorm=1                # TODO: enable BN\n",
    "    ),\n",
    "    keras.optimizers.SGD(\n",
    "        learning_rate=lr,\n",
    "        momentum=m                     # TODO: keep momentum fixed\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build + compile model WITHOUT Batch Normalization\n",
    "model_no_bn = compile_model(\n",
    "    build_deep_mlp(\n",
    "        input_dim=input_dim,\n",
    "        depth=d,\n",
    "        width=w,\n",
    "        activation=activ,\n",
    "        kernel_initializer=base_init,\n",
    "        use_batchnorm=0                # TODO: disable BN\n",
    "    ),\n",
    "    keras.optimizers.SGD(\n",
    "        learning_rate=lr,\n",
    "        momentum=m\n",
    "    )\n",
    ")\n",
    "\n",
    "# function call- do not change\n",
    "hist_bn = fit_model(model_bn, epochs=20)\n",
    "hist_no_bn = fit_model(model_no_bn, epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_no_bn.history[\"val_loss\"], label=\"No BN val_loss\")\n",
    "plt.plot(hist_bn.history[\"val_loss\"], label=\"With BN val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Batch Normalization effect on convergence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b65c5b",
   "metadata": {},
   "source": [
    "## Q9. Optimizer study (SGD vs Momentum vs Adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7617d-d500-40d8-b0d0-eed87f8e8d07",
   "metadata": {},
   "source": [
    "### Optimizer Comparison: SGD vs. Momentum vs. Adam (What You Should Do)\n",
    "\n",
    "In this step, you will compare how different **optimization algorithms** affect training and validation performance.\n",
    "\n",
    "You should:\n",
    "- Keep the experiment **fair and controlled** by fixing:\n",
    "  - the **same architecture** (depth, width, activation)\n",
    "  - the **same initialization** (`base_init`)\n",
    "  - BatchNorm setting (here: `use_batchnorm=False`)\n",
    "- Train three models using different optimizers:\n",
    "  - **SGD** (learning rate `1e-2`, momentum `0.0`)\n",
    "  - **SGD + Momentum** (learning rate `1e-2`, momentum `0.9`)\n",
    "  - **Adam** (learning rate `1e-3`)\n",
    "\n",
    "After training:\n",
    "- Plot **validation loss vs. epoch** for all optimizers on the same figure.\n",
    "- Compare which optimizer reaches low validation loss faster and which is more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e566f6-1ea0-4fbd-b056-caae6de8a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimizer Comparison — keep architecture + initialization fixed\n",
    "\n",
    "# TODO: Create a shared architecture config dictionary (same for all optimizers)\n",
    "arch_kwargs = dict(\n",
    "    input_dim=input_dim,\n",
    "    depth=d,\n",
    "    width=w,\n",
    "    activation=activ,\n",
    "    kernel_initializer=base_init,      # TODO: base_init from earlier\n",
    "    use_batchnorm=1            # TODO: keep BN fixed (True or False)\n",
    ")\n",
    "\n",
    "# TODO: Build + compile models with different optimizers (architecture must be identical)\n",
    "\n",
    "model_sgd = compile_model(\n",
    "    build_deep_mlp(**arch_kwargs),\n",
    "    keras.optimizers.SGD(\n",
    "        learning_rate=lr,\n",
    "        momentum=0\n",
    "    )\n",
    ")\n",
    "\n",
    "model_mom = compile_model(\n",
    "    build_deep_mlp(**arch_kwargs),\n",
    "    keras.optimizers.SGD(\n",
    "        learning_rate=lr,\n",
    "        momentum=m\n",
    "    )\n",
    ")\n",
    "\n",
    "model_adam = compile_model(\n",
    "    build_deep_mlp(**arch_kwargs),\n",
    "    keras.optimizers.Adam(\n",
    "        learning_rate=lr\n",
    "    )\n",
    ")\n",
    "\n",
    "# function call- do not change\n",
    "hist_sgd = fit_model(model_sgd, epochs=20)\n",
    "hist_mom = fit_model(model_mom, epochs=20)\n",
    "hist_adam = fit_model(model_adam, epochs=20)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist_sgd.history[\"val_loss\"], label=\"SGD val_loss\")\n",
    "plt.plot(hist_mom.history[\"val_loss\"], label=\"SGD+Momentum val_loss\")\n",
    "plt.plot(hist_adam.history[\"val_loss\"], label=\"Adam val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Optimizer comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e40e6f",
   "metadata": {},
   "source": [
    "## Q10. Final evaluation (test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f4624-96c7-47d2-92c9-1a9491515cfe",
   "metadata": {},
   "source": [
    "### Final Model Selection & Evaluation (What You Should Do)\n",
    "\n",
    "In this step, you will **select the best-performing model** from your previous controlled experiments and evaluate its performance.\n",
    "\n",
    "You should:\n",
    "- Review results from all prior experiments (initialization, regularization, BatchNorm, optimizers).\n",
    "- **Choose ONE final model** that demonstrates the best balance of:\n",
    "  - stable convergence,\n",
    "  - low validation loss,\n",
    "  - good generalization.\n",
    "- Assign this model to `final_model`.\n",
    "\n",
    "Then:\n",
    "- Use the provided `evaluate(...)` function to compute performance on:\n",
    "  - the **validation set**, and\n",
    "  - the **held-out test set**.\n",
    "- Report both **loss** and **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c994a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: keras.Model, X, y):\n",
    "    y = y.reshape(-1, 1).astype(\"float32\")\n",
    "    return dict(zip(model.metrics_names, model.evaluate(X, y, verbose=0)))\n",
    "\n",
    "# TODO: Choose the best model based on your controlled experiments.\n",
    "final_model = model_adam\n",
    "\n",
    "val_results = evaluate(final_model, X_val_p, y_val_i)\n",
    "test_results = evaluate(final_model, X_test_p, y_test_i)\n",
    "\n",
    "print(\"Validation:\", val_results)\n",
    "print(\"Test:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae8e04-5471-4d25-a5af-9a41401b7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the evaluation utility and final model selection\n",
    "\n",
    "def evaluate(model: keras.Model, X, y):\n",
    "    # TODO: reshape labels to (N, 1) and cast to float32\n",
    "    y = y.reshape(-1, _____).astype(_____)\n",
    "\n",
    "    # TODO: evaluate the model silently and return a dictionary of metrics\n",
    "    return dict(\n",
    "        zip(\n",
    "            model._____,                   # TODO: metric names\n",
    "            model._____(X, y, verbose=_____)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: Select the best-performing model based on your controlled experiments\n",
    "# (initialization, regularization, BatchNorm, and optimizer)\n",
    "final_model = _____\n",
    "\n",
    "\n",
    "# TODO: Evaluate the final model on validation and test sets\n",
    "val_results  = evaluate(_____, _____, _____)\n",
    "test_results = evaluate(_____, _____, _____)\n",
    "\n",
    "print(\"Validation:\", val_results)\n",
    "print(\"Test:\", test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3cd183-e286-481d-944f-386bd23ad70c",
   "metadata": {},
   "source": [
    "## Results & Discussion (Answer Each Question Clearly)\n",
    "\n",
    "- **Review and answer the following questions carefully briefly**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175b587-f98a-4ba5-b079-19095f2583eb",
   "metadata": {},
   "source": [
    "## Q11. Gradient Behavior Analysis \n",
    "\n",
    "From the gradient‐norm computation code, do gradients in early layers **vanish, explode, or remain stable** for naïve initialization compared to Xavier/He? State your conclusion using observed gradient norms.\n",
    "\n",
    "**Type your answer:** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f876cb-9587-413c-b2fa-873c1fa0fa34",
   "metadata": {},
   "source": [
    "## Q12. Initialization and Convergence  \n",
    "\n",
    "Based on the validation-loss curves for Xavier (tanh) and He (ReLU) initializations, compare their convergence speed and final performance. Which combination performed better in this deep MLP, and why is this result theoretically expected?\n",
    "\n",
    "**Type your answer:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743482b-29b9-4522-8ff3-a75bf6659774",
   "metadata": {},
   "source": [
    "## Q13. Regularization and Generalization  \n",
    "\n",
    "Analyze the effects of L2 weight decay and Dropout on training and validation performance. How did different regularization strengths affect overfitting, convergence, and validation loss? Which regularization setting provided the best generalization, and why?\n",
    "\n",
    "**Type your answer:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec0519-b88f-442c-8a32-0ad208eb7659",
   "metadata": {},
   "source": [
    "## Q14. Optimization and Training Stability\n",
    "\n",
    "Compare the behavior of SGD, SGD with momentum, and Adam using the validation-loss curves. Discuss differences in convergence speed, stability, and generalization. How did Batch Normalization interact with these optimizers to influence training dynamics?\n",
    "\n",
    "**Type your answer:**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afcc039-72b8-4114-bf13-fb171c0d2180",
   "metadata": {},
   "source": [
    "### 🎉 Congratulations!\n",
    "\n",
    "You have successfully completed **Assignment 2**. Excellent work engaging with core concepts in **deep neural networks**, including **gradient behavior**, **weight initialization**, **regularization**, **batch normalization**, and **optimization strategies** for training multi-layer perceptrons.\n",
    "\n",
    "### **Submission Instructions**\n",
    "\n",
    "Please submit a **GitHub repository link** on Canvas that contains:\n",
    "- The **completed Jupyter notebook**\n",
    "- Any additional files required for the assignment (if applicable)\n",
    "\n",
    "Before submitting, ensure that:\n",
    "- All **code cells (Q2–Q10)** have been executed successfully\n",
    "- All **Markdown responses (Q11–Q14)** have been completed\n",
    "- The notebook is **saved after execution** so that outputs are visible\n",
    "\n",
    "Once verified, **push the final version to GitHub** and submit the repository link on Canvas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
